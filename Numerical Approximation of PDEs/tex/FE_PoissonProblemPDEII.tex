% -*- root: ../NumericalApproximationofPDEs.tex -*-

\subsection{Numerical approximation - Galerkin method}

In this section we will study the numerical approximation of the Poisson problem by the Galerkin problem we saw in \fref{sec:Theory:GalerkinApprox}, centering too in the computational complexity of solving this problem.

As always, we can consider a finite element space $V_h ∈ H_0^1(Ω)$ spanned by a finite number of basis functions $\set{φ_i}_{i = 1}^N$ and we try to solve the problem there. As we have a finite element space, we can consider the coefficients of the functions as a vector $\vu ∈ ℝ^N$ so that the bilinear form can be expressed in matrix form: \[ a(u_h, v_h) = \sum_{i,j = 1}^N u_i · v_i \underbracket{a(φ_i, φ_j)}_{A_{ij}} = \trans{\vu} \mA \vv \]

This allows us to see that $\mA$ is positive definite and invertible. For the first, we can see that coercivity gives it directly: $\vv^T \mA \vv = a(v_h, v_h) ≥ α \norm{v_h}^2$. For invertibility, if $\trans{\vv} \mA \vv = 0$, then $a(v_h, v_h) = 0$ but by coercivity $a(v_h, v_h) ≥ \norm{v_h}^2$ so we need $v_h = 0$. Thus, $\ker \mA = \set{0}$ and it is invertible.

This allows us to formulate the following proposition for the bounding of the eigenvalues\footnote{Which is just a problem-specific version of \fref{thm:Theory:ConditioningMatrix}.}, which in turn will let us give bounds for the time required to solve this problem.

\begin{prop} \label{prop:PDE:BoundsMatrix} Let $\mA ∈ ℝ^{N×N}$ be the stiffness matrix of the Poisson problem \eqref{eq:PDE:PoissonProblem} defined by $A_{ij} = \int_Ω ∇φ_i ∇φ_j$. Then, there exists two constants $C_1, C_2 > 0$ such that \[ C_1 \vv^T \mM \vv ≤ \vv^T \mA ≤ \frac{C_2}{h^2} \vv^T \mM \vv \] with $\mM$ the mass matrix given by $M_{ij} = \int_Ω φ_i φ_j$.
\end{prop}

For the proof of this proposition we will need the inverse inequality.

\begin{prop}[Inverse inequality][Inequality!inverse] \label{prop:PDE:InverseInequality} Let $V_h ∈ H_0^1(Ω)$ be a finite element space defined on a \nref{def:Theory:RegularFamilyMeshes} \mesh. Then, for all $h > 0$ and for any $K ∈ \mesh$ the following holds: \[ \int_K \abs{∇v_h}^2 ≤ \frac{C}{h_K^2} \int_K v_h^2 \]
\end{prop}

\begin{proof}
\end{proof}

\begin{proof}[\Fref{prop:PDE:BoundsMatrix}] First, by using \nref{thm:Fund:PoincareInequality}, we know that there exists a constant $C_p > 0$ such that $\norm{v}_{L^2} ≤ C_p \norm{∇v}_{L^2}$ which gives us the bound \[ \vv^T \mM \vv ≤ C_p^2 \vv^T \mA \vv \]

For the other bound, we use the \nref{prop:PDE:InverseInequality} summing over all mesh elements $K ∈ \mesh$ and using the inverse assumption that $Ch ≤ h_k ≤ h$, so that \[ \int_Ω \abs{∇v_h}^2 ≤ \frac{C}{h^2} \int_Ω v_h^2 \] which gives directly the matrix bound.
\end{proof}

As discussed in \fref{sec:Theory:ConditioningNumber}, the numerical method to solve these problems will be preconditioned conjugate gradient to avoid memory storage problems. This method has the following property:

\begin{prop} Let $\vu^k$ be the approximation obtained by the preconditioned conjugate gradient method of the porblem $\mA \vu = \vf$ after $k$ steps, with $\mM$ as a preconditioner. Then, the following error estimate holds: \[ \norm{\vu - \vu^k}_{CG} ≤ 2 \norm{\vu - \vu^0}_{CG} \left(\frac{\sqrt{K(\inv{\mM}\mA)} - 1}{\sqrt{K(\inv{\mM}\mA)} + 1}\right)^k\] with $\norm{·}_{CG}$ defined as follows: \[ \norm{\vv}_{CG} = \]
\end{prop}

With the bounds explained in \fref{prop:PDE:BoundsMatrix} we can bound the condition number: \[ C_1 ≤ \inf_{\vv ∈ ℝ^N \setminus\set{0}} \frac{\vv^T \mA \vv}{\vv \mM \vv} ≤ λ_j = \frac{\vx_j^T \mA \vx_j}{\vx_j^T \mM \vx_j} ≤ \sup_{\vv ∈ ℝ^N \setminus\set{0}} ≤ \frac{C_2}{h^2}\] so that $K(\inv{\mM} \mA) ≤ \frac{C_2}{C_1h^2}$.

Thus, in order to reduce the initial error by a factor of $ε$, we see that \[ ε\norm{\vu - \vu^0}_{CG} = \norm{\vu - \vu^k}_{CG} ≤ 2 \norm{\vu - \vu^0}_{CG} \left(\frac{\sqrt{K(\inv{\mM}\mA)} - 1}{\sqrt{K(\inv{\mM}\mA)} + 1}\right)^k = 2 \norm{\vu - \vu^0}_{CG} ( 1 - \mathcal{O}(kh))\] so that the number $k$ of iterations is $k = \mathcal{O}(\sfrac{1}{h})$.

\subsection{A priori error estimates}

This section is nothing specially new, as it is just applying the results of \fref{sec:Theory:ApproxResults}.

\begin{prop} Given a \nref{def:Theory:RegularFamilyMeshes} with aspect ratio bounded by $γ ∈ ℝ^+$ for a convex polygon, then there exists a constant $C >0$ independent of $h$ and $u$ such that \[ \norm{u - u_h}_{L^2} + h \norm{∇(u - u_h)}_{L^2} ≤ Ch^2 \norm{∇^2 u}_{L^2} \]
\end{prop}

\begin{proof} We will prove separately the two estimates.

\proofpart{Bound for $\norm{∇u - ∇u_h}_{L^2}$}

By definition, we know that $\norm{∇u - ∇u_h}_{L^2} = \int_Ω ∇(u - u_h) · ∇(u - u_h)$. By \nref{def:Theory:GalerkinOrthog}, we know that, for any $v_h ∈ V_h$,  $\int_{Ω} ∇(u - u_h) · ∇v_h = a(u - u_h, v_h) = 0$ so that \[ \norm{∇u - ∇u_h}_{L^2} = \int_Ω ∇(u - u_h) · ∇(u - u_h) ≤ \norm{∇(u - u_h)}_{L^2} \norm{∇(u - v_h)}_{L^2}\]

The trick is now choosing the correct $v_h$, and we will select the Lagrange interpolant given by \[ v_h = I_h u = \sum_{j=1}^N u(p_j) φ_j \]

Since $Ω$ is a convex polygon, $u ∈ H^2(Ω)$  and therefore for $u ∈ C^0(\adh{Ω})$ we choose $v_h = I_h u$. Then we can use the \nref{thm:Theory:InterpErrorSmooth} and finally $\norm{∇(u - u_h)}_{L^2} ≤ \norm{∇(u - I_hu)}_{L^2} ≤ Ch \norm{∇^2 u}_{L^2}$.
\end{proof}

\subsection{A posteriori error estimates}

The a posteriori error estimates will give us error estimates after the approximate solution has been computed.

\begin{prop}[A posteriori error\IS for the Poisson problem] Given a shape regular mesh then there exists a constant $C_1 > 0$ independent of $h,u,f$ but depending on the mesh aspect ratio such that \( \norm{∇(u - u_h)}_{L^2(Ω)}^2 ≤ C_1 \sum_{K ∈ \mesh} η_K^2 \label{eq:PDE:APostGradError} \) where $η_K = h_K \norm{Δu_h + f}_{L^2(K)} + h_K^{\sfrac{1}{2}} \norm{[∇u_h]}_{L^2(∂K)}$ and $[∇u_h]$ is the jump of $∇u_h$ in the boundary between two mesh elements $K$ and $K'$ given by $[∇u_h] = \restr{∇u_h}{K} - \restr{∇u_h}{K'}$

Moreover, if $Ω$ is a convex polygon then there exists another constant $C_2$ independent of $h,u,f$ but depending on the mesh aspect ratio such that \( \norm{u - u_h}_{L^2(Ω)^2} ≤ C_2 \sum_{K ∈ \mesh} h_K^2 η_K^2 \label{eq:PDE:APostError} \)
\end{prop}

\begin{proof}

\proofpart{\eqref{eq:PDE:APostGradError}}
% TODO: Fix this crappy proof.

So something something \[ \norm{∇(u - u_h)}_{L^2}^2 = \int_Ω ∇(u - u_h) ∇(u - u_h) = \int_Ω f(u - u_h) - ∇u_h ∇(u - u_h)\] where the last part is $\dualp{\mop{Residual}\, u_h, u - u_h}$. We can substract any test function $v_h ∈ V_h$ so that everything is still the same in \[ \sum_{K ∈ \mesh} \int_K f(u - u_h - v_h) - ∇u_h ∇(u - u_h - v_h)\]

Integrating by parts, we are left with \[ \int_K f(u - u_h - v_h) - ∇u_h ∇(u - u_h - v_h) = \int_K (f + Δu_h)(u - u_h - v_h) - \int_{∂K} ∇u_h · \vn (u - u_h - v_h) \] and now the trick is noticing something and other thing and using Cauchy-Scharwz we have that \begin{multline*}
\int_K (f + Δu_h)(u - u_h - v_h) - \int_{∂K} ∇u_h · \vn (u - u_h - v_h) ≤ \\ ≤ \norm{f + Δu_h}_{L^2(K)} \norm{u - u_h - v_h}_{L^2(K)} + \frac{1}{2} \norm{[∇u_h · \vn]}_{L^2(∂K)} \end{multline*}

We use the Clément interpolant so that $v_h = R_h(u - u_h)$, which will gives us $\norm{v - R_h v}_{L^2(Ω)} ≤ Ch \norm{∇v}_{L^2(Ω)}$ which does not hold for the Lagrange interpolant in dimensions greater than 1.

The only issue here is that we have a function $v ∈ H_0^1$ so we don't have a point value for the interpolant $v(p_j)$. Luckily, we can average so that \[ R_h v(p_j) = \sum_{\substack{K ∈ \mesh \\ p_j ∈ K}} \fint_K v \]

Then the interpolation result is \[ \frac{1}{h_k^2} \norm{v - R_hv}^2_{L^2(K)} + \norm{∇(v - R_hv)}_{L^2(K)} + \frac{1}{h_K} \norm{v - R_hv}_{L^2(∂K)} ≤ \norm{∇v}_{L^2(ΔK)} \]

\proofpart{\eqref{eq:PDE:APostError}}

We define $\appl{φ}{Ω}{ℝ}$ such that \[ \begin{cases} -Δφ = u - u_h & \text{in } Ω \\ φ = 0 & \text{on } ∂Ω \end{cases} \]

This is a PDE problem that can be translated to a weakform, so that \[ \int_Ω ∇φ ∇v = \int_Ω (u - u_h) v \qquad ∀v ∈ H_0^1(Ω)\]

Since $Ω$ is convex, then $φ ∈ H^2(Ω)$ and $\norm{φ}_{H^2(Ω)} ≤ C\norm{u-u_h}_{L^2}$. We can then calculate \begin{align*} \int_Ω (u - u_h)^2 &= \int_Ω ∇φ∇(u - u_h) = \int_Ω fφ - ∇u_h ∇h = \dualp{\mop{residual}(u_h), φ} \\
&= \int_Ω (φ - v_h) - ∇u_h ∇ (φ - v_h) \qquad ∇v_h ∈ V_h \\
&= \sum_{K ∈ \mesh} \int_K (f + Δu_h)(φ - v_h) + \frac{1}{2} \int_{∂K} [∇u_h · \vn] (φ - v_h)
\end{align*}

Now we have to choose $v_h$, and we will use $v_h = R_h φ$ as $φ ∈ C^0(Ω)$ because it is in $H^2(Ω)$. Then we can continue bounding by Cauchy-Scharwz:
\begin{align*}
\int_Ω (u - u_h)^2 &≤ C \sum_{K ∈ \mesh} \left(h_k^2 \norm{f + Δu_h}_{L^2(K)} + \frac{1}{2} h_k^{\sfrac{3}{2}} \norm{[∇u_h · \vn]}_{L^2(∂K)} \right) \norm{∇^2 φ}_{L^2(K)} \\
	&≤ C\left(\sum_{K ∈ \mesh} h_k^2 η_K^2\right)^{\sfrac{1}{2}} \norm{u - u_h}_{L^2(Ω)}
\end{align*} obtaining thus the result.

\end{proof}

% TODO
Some explanation of $η_K$ and more things.

\subsubsection{Gradient a posteriori error}
\label{sec:PDE:GradientAPosterioriElliptic}

The idea of the following proposition is that we can compute a gradient operator and we will have guarantees that it converges to the real error (which we cannot compute).

\begin{prop} Assumme that there is a \concept[Gradient!post-processed]{post-processed gradient} $\appl{\gradop }{V_h}{V_h}$, two constants $C_1, C_2 > 0$  and two exponents $0 < α < β ≤ 2$ such that for all $h > 0$ we have \begin{align*}
\norm{∇u - \gradop u_h}_{L^2(Ω)} &≤ C_1 h^β \\
\norm{∇u - ∇u_h}_{L^2(Ω)} &≥ C_2 h^α
\end{align*}

Then, $\norm{∇u - \gradop u_h}_{L^2(Ω)} \convs[][h][0] \norm{∇u - ∇u_h}_{L^2(Ω)}$.
\end{prop}

\begin{proof} By using the triangle inequality, we can see that $\norm{∇u_h - \gradop u_h}_{L^2} ≤ \norm{∇u - ∇u_h}_{L^2} + \norm{∇u - \gradop u_h}_{L^2}$ so that \[ \frac{\norm{∇u_h - \gradop u_h}}{\norm{∇u - ∇u_h}} ≤ 1 + \frac{C_1}{C_2} h^{β - α} \convs[][h][0] 1\]

On the other hand, we have that \[ \frac{\norm{\gradop  u_h - ∇u_h}}{\norm{∇u - u_h}} ≥ 1 - \frac{\norm{∇u - \gradop u_h}}{\norm{∇u - ∇u_h}} ≥ 1 - \frac{C_1}{C_2} h^{β - α} \convs[][h][0] 1\]
\end{proof}

The interesting question is how to build the better gradient operator $\gradop $. The idea is to use the Clément interpolant of $∇u_h$, so that $\gradop  u_h(x) = \sum_{j = 1}^N \gradop u_h(p_j) φ_j(x)$ with \[ \gradop u_h (p_j) = \frac{\displaystyle\sum_{\substack{K ∈ \mesh \\ p_j ∈ K}} \int_K ∇u_h}{\displaystyle\sum_{\substack{K ∈ \mesh \\ p_j ∈ K}} \int_K 1} \] as the averages of the values in the point $p_j$ evaluated on all the mesh elements $K$ containing $p_j$.

On parallel meshes\footnote{Drawing of a mesh with rectangular triangle mesh elements.} it can be proved that if $u ∈ H^3(Ω)$ then $\norm{∇u - G u_h}_{L^2} = \oof{h^2}$ (superconvergence). On structured meshes\footnote{I think he said paralel meshes with small modifications, such a bending or something like that.} we hate $\norm{∇u - Gu_h} = \oof{h^β}$ with $1 < β ≤ 2$.

Proofs of these facts are terribly tedious.

\subsubsection{Goal-oriented estimations}

In some cases we will be interested in computing not the approximation error but another quantity depending on the error, such as $\int ρ (u - u_h) $ for some function ρ. For example, this could be an indicator function so that we measure the error only in a region of interest.

In order to perform this estimation, we consider generally a functional $J(v) = \int_Ω ρ v$, so that we want to estimate $J(u - u_h)$. We will introduce a \textit{dual problem}, that of finding $\appl{φ}{Ω}{ℝ}$ such that \[ \begin{cases} -Δφ = ρ & \text{in } Ω \\ φ = 0 & \text{on } ∂Ω \end{cases} \]

This problem has a solution $φ ∈ H_0^1(Ω)$ such that \[ \int_Ω ∇φ ∇v = \underbracket{\int_Ω ρv}_{ = J(v)} \quad ∀v ∈ H_0^1(Ω)\]

Thus, we have that \[ J(u - u_h) = \int_Ω ∇φ ∇(u - u_h) = \int_Ω fφ - ∇u_h ∇φ = \dualp{\mop{residual}(u_h), φ} \]

By using the \nref{def:Theory:GalerkinOrthog}, we can substract any $v_h ∈ V_h$ to $φ$ and the result holds:
\begin{align*}
J(u - u_h) &= \int_Ω \left[f (φ - v_h) - ∇u_h ∇(φ - v_h)\right] = \\
	&= \sum_{K ∈ \mesh} \int_K (f + Δu_h)(φ - v_h) + \underbracket{\frac{1}{2} \int_K [∇u_h · \vn](φ - v_h)}_{= 0 \text{ because reasons}}
\end{align*}

Now the issue is to choose a correct test function $v_h$. Let $φ_h ∈ V_h$ be a finite element approximation to $φ$ (that is, $\int_Ω ∇φ_h ∇v_h = \int_Ω ρ v_h$ for all $v_h ∈ V_h$), so that then we choose $v_h = φ_h - R_h(φ - φ_h)$ where $R_h$ is the Clément interpolant operator. With this, we can finally give a bound, using Cauchy-Schwarz and interpolation results
\begin{align*}
J(u - u_h)
	&≤ \sum_{K ∈ \mesh} \norm{f + Δu_h}_{L^2(K)} \norm{φ - φ_h - R_h(φ - φ_h)}_{L^2(K)} \\
	&\qquad + \frac{1}{2}\norm{[∇u_h· \vn]}_{L^2(∂K)}\norm{φ - φ_h - R_h(φ - φ_h)}_{L^2(∂K)} \\
 	&≤ C \sum_{K ∈ \mesh} \left( h_K \norm{f + Δu_h}_{L^2(K)} + \sqrt{h_K} \norm{[∇u_h·\vn]}_{L^2(∂K)}\right) \norm{∇(φ - φ_h)}_{L^2(ΔK)}
 \end{align*} which finally gives a convergence rate $J(u - u_h) = \oof{h^2}$.

The only problem is that we have to guess $\norm{∇(φ - φ_h)}_{L^2(ΔK)}$. Luckily, we can use the post-processed \gradop we defined in \fref{sec:PDE:GradientAPosterioriElliptic} and then compute $\norm{\gradop φ_h - ∇φ_h}_{L^2(ΔK)}$. Under some conditions, this will be a good enough appxorimation. Another option would be to compute a quadratic approximation of $φ$ and use its gradient for the approximation of $∇φ$.

\subsection{Adaptive algorithm}

The idea of adaptive algorithms is to build a mesh \mesh such that the relative error is close to a preset tolerance $\text{Tol}$ (usually $10\%$ error) so that \[ \frac{\norm{∇(u - u_h)}_{L^2(Ω)}}{\norm{∇u_h}_{L^2(Ω)}} \approx \text{Tol} \]

Of course, that is not computable so we will use an approximation by using the interpolation constant. Also, we will specify what we mean by ``close to the tolerance''. Thus, the goal is to estimate \( 0.5 · \text{Tol} ≤ \frac{\left(\sum_{K ∈ \mesh} η_K^2\right)^{\sfrac{1}{2}}}{\norm{∇u_h}_{L^2(Ω)}} ≤ 1.5 · \text{Tol} \) and, in order to make things easier, we will just try to satisfy \( \frac{0.5^2 · \text{Tol}^2 \norm{∇u_h}^2_{L^2(Ω)}}{N_k} ≤ η_K^2 ≤ \frac{1.5^2 · \text{Tol}^2 \norm{∇u_h}^2_{L^2(Ω)}}{N_k} \qquad ∀K ∈ \mesh \label{eq:PDE:AdaptiveErrorCond} \) with $N_k$ being the number of triangles in the mesh.

\begin{algorithm}[hbtp]
\begin{algorithmic}
\Function{RefineMesh}{$\mesh^0$}
\State stop $\gets$ False
\State $\mesh \gets \mesh^0$
\While{stop == False}
	\State stop $\gets$ True
	\State lowerBound $\gets \frac{0.5^2 · \text{Tol}^2 \norm{∇u_h}^2_{L^2(Ω)}}{N_k}$
	\State upperBound $\gets \frac{1.5^2 · \text{Tol}^2 \norm{∇u_h}^2_{L^2(Ω)}}{N_k}$
	\For{$K ∈ \mesh$}
		\If{$η_K^2 < \text{lowerBound}$}
			\State $\mesh \gets$ \Call{Coarsen}{$K,\, \mesh$}
			\State stop $\gets$ False
		\ElsIf{$η_K^2 > \text{upperBound}$}
			\State $\mesh \gets$ \Call{Refine}{$K,\, \mesh$}
			\State stop $\gets$ False
		\EndIf
	\EndFor
\EndWhile
\State \Return $\mesh$
\EndFunction
\end{algorithmic}
\caption{Refinement algorithm for adaptive meshes.}
\label{lst:RefinementAlgorithm}
\end{algorithm}

We are going to present one possible algorithm (see \fref{lst:RefinementAlgorithm}), but there are many other in the literature. We will generate a series $\set{\mesh^n}_{n ∈ ℕ}$ of meshes starting from an initial one $\mesh^0$. We will compute $u_h^n$ on each mesh $\mesh^n$ and $η_K$ for each element of the mesh $\mesh^n$. We will check whether \eqref{eq:PDE:AdaptiveErrorCond} holds for each element $K ∈ \mesh^n$. If it doesn't hold, the mesh should be refined in the neighbourhood of $K$. If, on the other hand, $η_K$ is too small, the mesh should be coarsened.

Note that this algorithm is not guaranteed to reach the goal. There is a theory about adaptive mesh convergence starting from a coarse mesh and only performing refinements.

\begin{figure}[hbtp]
\centering
\inputtikz{MeshRefinement}
\caption{Mesh refinement procedure. The highlighted triangle is the one due to be refined in each stage.}
\label{fig:MeshRefinement}
\end{figure}

\begin{figure}[hbtp]
\centering
\inputtikz{MeshCoarsening}
\caption{Mesh coarsening procedure. The highlighted triangles are the ones going to be coarsened.}
\label{fig:MeshCoarsening}
\end{figure}

For the refinement process, there are several possibilities. For refinement, we bisect the triangle to refine by the largest edge. Then, if the mesh is not acceptable, we refine neighbouring mesh elements until the mesh becomes conformal again. An example procedure can be seen in \fref{fig:MeshRefinement}.

For coarsening, we remove the vertex and the corresponding edges which produces the largest angles as can be seen in \fref{fig:MeshCoarsening}.

\begin{defn}[Delaunay\IS property] A triangulation $\mesh$ satisfies the Delaunay property if $∀K ∈ \mesh$ its circumscribed circle tontains no vertex in its interior.

An equivalent statement (only in 2D) is that a triangulation is Delaunay if for any internal edge, the sum of the two opposite angles is less or equal than π.
\end{defn}

\begin{corol} Given a set of points in $ℝ^2$, consider all the conformal triangulations of the convex hull. Then a Delaunay triangulation will maximize the smallest angle.
\end{corol}

\begin{prop} Let $\mA$ be the stiffness matrix given by $A_{ij} = \int_Ω ∇φ_i ∇φ_j \dif x$. If the mesh \mesh is of Delaunay type, then $\mA$ is monotone: if $\mA \vx ≥ \vec{0}$ (that is, all components are positive) then $\vx ≥ \vec{0}$.
\end{prop}

This is important if we have, for example, a Laplace equation $\mA \vx = \vf$. If the force is positive, we want to ensure that the deformation is also positive.

This is also usually called the maximum principle. However, we cannot guarantee that once we go to the finite element method we are going to have the same property.

\begin{figure}[hbtp]
\centering
\inputtikz{MeshRefinementDelaunay}
\caption{Delaunay triangulation refinement procedure. In this case we add two vertices, with the new edges highlighted in green, and the ones removed in red.}
\label{fig:MeshRefinementDelaunay}
\end{figure}

If we want to add a vertex $P$ to a Delaunay triangulation maintaining that property, we have to do it with care. We will consider all the triangles whose circumscribed circle contains $P$. We take the union of all those triangles and remove the internal edges.  Then we link the vertex to the vertices of the polygon.
