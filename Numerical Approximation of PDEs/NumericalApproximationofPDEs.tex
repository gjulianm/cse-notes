\documentclass[palatino]{epflnotes}

\title{Numerical Approximation of PDEs}
\author{Guillermo Julián Moreno}
\date{16/17 - Fall semester}

% Additional packages
\usepackage{tikztools}
% --------------------

\begin{document}
\frontmatter
\pagestyle{plain}
\maketitle

\tableofcontents
\mainmatter
% Content

\chapter{Functional analysis fundamentals}

\section{Banach and Hilbert spaces}

The first fundamental of functional analysis is, as usual, to define the basic spaces in which we are going to work. These are going to be, obviously, spaces of functions. The next step is what do we want to study in these spaces. The first notion is that of convergence: knowing when a sequence of functions converges to another and in which sense. Instead of going like in other courses defining special types of convergence, we are actually going to rely on topology. We will treat functions like points in a topological space, and saying that they converge to something if they start getting closer to that something.

So, what we need is a distance or, actually, a norm\footnote{A norm $\norm{·}$ induces a distance directly by $\dst(x,y) = \norm{x-y}$.}. Let's go with the definition:

\begin{defn}[Norm] Given a space $V$ over a field $K$, we say that an application $\appl{\norm{·}}{V}{ℝ}$ is a norm if and only if
\begin{enumerate}
	\item Given $λ ∈ K$ and $x ∈ V$, then $\norm{λx} = \abs{λ}\norm{x}$.
	\item Verifies the triangle equality: given $x,y ∈ V$, then $\norm{x + y} ≤ \norm{x} + \norm{y}$.
	\item It separates points, that is, $\norm{x} = 0 \iff x = 0$.
\end{enumerate}

If the last property is not verified, we say we only have a \concept{Seminorm}.
\end{defn}

We will then work in spaces equipped with a norm. We will ask that these spaces have a nice property with respect to this norm: completeness. We say that a space $V$ is \concept[Completeness]{complete} if and only if for every Cauchy sequence $(x_n) ⊂ V$, we have convergence in the space $x_n \to x ∈ V$. In other words, we don't want to worry whether we are going to be out of the space when working with convergent sequences.

With this, we can go on to the notion of a Banach space:

\begin{defn}[Banach space][Space!Banach] A vector space $V$ is called Banach if it has a norm and is complete with respect to that norm. Formally, the Banach space is the space with the norm, that is, $X = (V, \norm{·}_V)$.
\end{defn}

Banach spaces are the ones we do topology in. Notions of compactness and convergence\footnote{Mainly the two topological aspects we will be interested in: compactness gives us finiteness and convergence, duh, is convergence.} will be studied on these spaces.

The next step is to be able to do geometry, measuring angles and specially talking about ortogonality. This is accomplished with the scalar product:

\begin{defn}[Scalar product][Product!scalar] Given a vector space $V$, we say that an application $\appl{\pesc{·,·}}{V×V}{ℝ}$ is a scalar product if and only if it is antilinear\footnote{Beware with complex numbers: $\pesc{λx,y} = λ\pesc{x,y}$ but $\pesc{x,λy} = \conj{λ}\pesc{x,y}$.} , positive definite and symmetric\footnote{Again, take into account that with complex numbers we have $\pesc{x,y} = \conj{\pesc{y,x}}$.}.
\end{defn}

Again, we have a name for spaces with a scalar product:

\begin{defn}[Hilbert space][Space!Hilbert] We say that a vector space with a scalar product $(V, \pesc{·,·}_V)$ if and only if it is Banach with respect to the induced norm $\norm{x}_V = \sqrt{\pesc{x,x}_V}$.

As it was the case with Banach space, the subjacent space and the tuple space-scalar product is often used interchangeably, but the good mathematician should know what is what.
\end{defn}

\subsection{$L^p$ spaces}

Now on to a specific example of Banach and Hilbert spaces: $L^p$ spaces. These will be our basic workbench, so it pays off to define them carefully.

The first question we have to make is which kind of functions do we want to study. One could think that, given that this is a subject on differential equations, we should be studying $C^k$ spaces for some $k ∈ ℕ$. However, this turns out to be too restrictive, and the reason is one of completeness: these spaces are too small. One can see that we can approximate absolute monster functions (and even things that are not even functions) with infinitely smooth functions. It would be certainly difficult to work in these kind of spaces where one can step out of them at the slightest oversight.

So, if we are dropping smoothness assumptions, we can work with the related operation: integration. Despite this horribly weak argument, this is the best choice although it requires a little bit of care and measure theory. Using integration yields nice complete spaces, a direct definition of a norm and a very good tolerance for monsters.

The first base is then to define the basic space. Given $Ω ∈ ℝ^n$ and a measure $μ$ on $ℝ$, we define the $\mathcal{L}^p$ space as \( \mathcal{L}^p(Ω, μ) ≝ \set{\appl{f}{Ω}{ℝ} \tq \int_Ω\abs{f}^p \dif μ < ∞ } \label{eq:Fundamentals:LCalSpace} \) with the corresponding norm \( \norm{f}_{\mathcal{L}^p(Ω,μ)} = \left(\int_Ω \abs{f}^p \dif μ \right)^{\sfrac{1}{p}} \label{eq:Fundamentals:LpNorm} \)

As the measure used is usually the Lebesgue measure, the `μ' is usually dropped off the notation.

What is important --- and the reason why we need measure theory --- is that the norm defined in \eqref{eq:Fundamentals:LpNorm} is not actually a norm for the space $\mathcal{L}^p$: it does not separate points. The function $f \equiv 0$ has obviously norm 0, but a function that doesn't vanish on a set of measure zero has norm 0 too.

The solution is a little bit `stupid', but it works: we define those two functions as to be the same one, and in fact we will do that with every two functions that are different only on a set of measure zero. With a little bit more formality, we define the following equivalence relation\footnote{Checking the properties is left as an exercise to the distrustful reader.}:
\( ∀f,g ∈ \mathcal{L}^p(Ω), \quad f \sim g \iff \int_Ω \abs{f-g} \dif μ = 0 \label{eq:Fundamentals:EquivRel} \)

With this we can define what is a $L^p$ space:

\begin{defn}[{$L^p$} space][Space!{$L^p$}] Let $Ω ⊂ ℝ^n$ be a domain, and $μ$ a measure on $ℝ$. Then, we define the $L^p(Ω,μ)$ space as the quotient $\quot{\mathcal{L}^p(Ω,μ)}{\sim}$, with the space defined as in \eqref{eq:Fundamentals:LCalSpace} and the equivalence relation of \eqref{eq:Fundamentals:EquivRel}.

In other words, $L^p$ space is the space of functions $p$-integrable on $Ω$, identifying those which only differ on a set of measure zero.

One special case: when $p = ∞$, the norm is given as \[ \norm{f}_{∞} = \sup_{x ∈ Ω} \abs{f(x)} \] and thus the $L^{∞}(Ω)$ space is the one of bounded functions in Ω.
\end{defn}

The $L^p$ spaces are always Banach for $0 < p ≤ ∞$. In the case $p = 2$, it is a Hilbert space with the scalar product being \[ \pesc{f,g}_{L^2(Ω)} = \int_Ω f g \dif μ \]

Proofs of these facts are left to the reader.

\section{Distributions}

In the previous section we explained how our space is going to be the one of integrable functions. However, we have not acknowledged something that is really important for PDEs: how can we differentiate general integrable functions?

This requires us to bring in the theory of distributions, which are actually useful not only because they will allow us to define ``derivatives'' of general functions, but also because they are a useful tool when solving PDEs.

A good motivation for distributions is given in \cite{DistributionsFourierTransform}, questioning how do we actually measure things. For example, when we have a thermometer, we don't measure the temperature in just one point in space: we do some kind of ``average'' between all the points of the mercury. An interesting thought experiment is to see what would happen if we measured the temperature of a zone in which there is one single point with infinite temperature. Could we measure that infinite? Or maybe we wouldn't see it because the other points would smooth it out?

Distributions can be seen as a model of these kind of things. We couldn't model a point with infinite value with a function, but maybe we could think of that as some kind of generalized function.

On to the mathematics, we will first define what our ``thermometers'' are going to be. The space of test functions will usually be defined as $\mathcal{D}(Ω) = C_0^∞(Ω)$; infinitely smooth functions with compact support on some open set $Ω ∈ ℝ^n$. This is one example of tradeoffs in functional analysis: by requiring the test functions to be infinitely smooth, we will be able to work with distributions without any kind of regularity property. If we relaxed the requirements on test functions, we would have a smaller class of distributions.

Now, what is a distribution? We will first have the formal definition.

\begin{defn}[Distribution] Let $Ω ⊂ ℝ^n$ be a domain, and $\mathcal{D}(Ω) = C_0^∞(Ω)$ our space of test functions. A distribution is then a continuous linear operator $\appl{T}{\mathcal{D}(Ω)}{ℝ}$.

The space of distributions on $\mathcal{D}(Ω)$ is denoted by $\mathcal{D}'(Ω)$.
\end{defn}

This is actually a direct translation of the motivation we were talking of before. A distribution is just something that, when measured with a test functions, gives us a number. This is usually denoted by $T(φ)$ or $\pesc{T, φ}$.

We require it to be linear because non-linear things are just awful. Continuity is also very important: if we are using a sequence of test functions converging to some other function, we want the result to be the same for the two possible paths of first measuring, then doing the limit and viceversa. Formally:

\begin{defn}[Continous operator][Continuity!of an operator] Given a Banach space $V$ and a linear operator $T$ on $V$, we say that it is continuous if and only if for every Cauchy sequence $(φ_n)_{n ≥ 1} ⊂ V$ with $φ_n \to φ$ we have that \[ \lim_{n\to ∞} \pesc{T,φ_n} = \pesc{T, φ_n}\]
\end{defn}

In the specific case of test functions $\dstr(Ω)$ with $Ω ∈ ℝ^d$, we have to be careful with the definition of convergence: given a sequence $(φ_n)_{n≥1} ⊂ \dstr(Ω)$, we say that it converges to $φ ∈ \dstr(Ω)$ if and only if $\sop φ_n ⊂ K$ for some compact set $K ⊂ Ω$, and if $∂^\vA φ_n \to ∂^\vA φ$ with respect to the supremum norm for every multiindex\footnote{Reminder: a multiindex $\vA = (α_1, α_2, \dotsc, α_d)$ is a notation for the mixed partial derivatives $\frac{∂^{\abs{\vA}}}{∂x_1^{α_1}∂x_2^{α_2} \dotsb ∂x_d^{α_d}}$.} $\vA ∈ ℕ^d$.

We have to see two examples of distributions. One will convince us that distributions are really generalized functions by seeing that functions are always distributions. Indeed, given $f ∈ \espLloc[1][Ω]$\footnote{Locally integrable functions, that is, funcitons integrable over every compact subset of Ω. This is a bigger class than $L^p$ (trivially, every integrable function in Ω is also integrable on any subset of it).}, we can assign it a distribution $T_f$ by means of \[ T_f(φ) = \int_Ω f φ \]

The second example convinces us that this space is actually bigger and possibly useful: we can define the \concept{Dirac's delta} centered in $x ∈ Ω$ as a distribution \[ \pesc{δ_x, φ} = φ(x)\] that gives us the value of the test function at $x$. It is easy to check that it is a distribution, but we couldn't define it as a function\footnote{We could, however, define it as a measure: given a set $E$, $δ_x(E) = 1$ if $x ∈ E$, and 0 if $x ∉ E$.} (it is zero everywhere but with an ``infinite impulse'' at $x$).

\subsection{Distributional derivatives}

After studying distributions, we can define derivatives on them. The motivation is to make this derivative coincide with the usual derivative when the distribution is associated to a function. So, let $f ∈ C^1(Ω)$, $T_f ∈ \dstr'(Ω)$ the associated distribution and $\dstr(Ω)$ our space of test functions. We would like that $T_f'(φ) = T_{f'}(φ)$ for all $φ ∈ \dstr(Ω)$. Writing it down and integrating by parts: \[ T_{f'}(φ) = \int_Ω f' φ \dif μ = \underbracket{\restr{f φ}{∂Ω}}_{=0} - \int_Ω fφ' \dif μ = - \int_Ω fφ' \dif μ \] where φ vanishes on the border of Ω as it must have compact support. This is enough to define the distributional derivative.

\begin{defn}[Distributional derivative][Derivative!distributional] Let $Ω ⊂ ℝ^d$ be an open set and $T ∈ \dstr'(Ω)$ a distribution. Then, we define the partial distributional derivative (with multiindex) $∂^\vA T$ as \[ \pesc{∂^\vA T, φ} = (-1)^{\abs{\vA}} \pesc{T, ∂^\vA φ} \]
\end{defn}

\section{Sobolev spaces}

Even though we have been able to define derivatives for arbitrary functions, we have the problem that not every $L^p$ function has a $L^2$ distributional derivative. For example, the function\footnote{Reminder: given a set $A$, we define the indicator or characteristic function as $\ind_A(x) = \begin{cases} 1 & x ∈ A \\ 0 & x ∉ A \end{cases}$.} $\ind_{ℝ^+}$ has distributional derivative $δ_0$, which is not even a function.

This is the motivation to define Sobolev spaces, spaces in which we will be able to do distributional derivatives without problems.

\begin{defn}[Sobolev space][Space!Sobolev] Let $Ω ⊆ ℝ^n$ be an open set. We define the Sobolev space of order $k ∈ ℕ$ and $1 ≤ p < ∞$ as the space of $L^p$ functions whose distributional derivatives are again $L^p$ functions. That is, \( W^{k,p} (Ω) ≝ \set{ u ∈ L^p(Ω) \tq ∂^\vA u ∈ L^p(Ω) \; ∀\abs{\vA} ≤ k} \)

Sobolev spaces are Banach spaces given the norm \( \norm{f}_{W^{k,p}} = \left(\sum_{\abs{\vA} ≤ k} \norm{∂^\vA u}_{L^p}^p\right)^{\sfrac{1}{p}} \label{eq:Fundamentals:NormSobolev} \) with the special case for $p = ∞$ \( \norm{f}_{W^{k,∞}} = \sup_{\abs{\vA} ≤ h} \norm{∂^\vA u}_{L^∞} \label{eq:Fundamentals:NormSobolevInfty} \)
\end{defn}

The special case is with $p = 2$, named as $H^k(Ω) = W^{k,2}(Ω)$, which is a Hilbert space with the inner product given by \[ \pesc{f,g}_{H^k(Ω)} ≝ \sum_{\abs{\vA} ≤ k} \int_Ω ∂^\vA f · ∂^\vA g \dif x \] which clearly induces the norm defined previously in \eqref{eq:Fundamentals:NormSobolev}.

For notational convenience, we can define the $H^k$ seminorm as simply by \[ \abs{f}_{H^k(Ω)}^2 = \sum_{\abs{\vA} = h} \norm{∂^\vA f}_{L^2(Ω)}^2\], and thus \[ \norm{f}_{H^k(Ω)}^2 = \sum_{m=0}^k \abs{f}^2_{H^m(Ω)} \]

\subsection{Embeddings of Sobolev spaces and regularity}

One important aspect of Sobolev spaces is ``where do they live'' or, in other words, in which spaces can we embed a given Sobolev space and what does that tell us about the regularity of the functions in it. This is solved by the Sobolev embedding theorem.

\begin{theorem}[Sobolev embedding theorem] Let $Ω$ be an open subset of $ℝ^d$ with a Lipschitz continuous boundary\footnote{A boundary is Lipschitz continuous if it is locally the graph of a function with \nref{def:LipschitzCont}.}. Then, the following embedding properties hold:
\begin{enumerate}
	\item If $0 ≤ 2k < d$, then $H^k(Ω) ⊂ L^q(Ω)$ with $q = \sfrac{2d}{(d-k)}$.
	\item If $2k = d$, then $H^k(Ω) ⊂ L^q$ for $2 ≤ q < ∞$.
	\item If $2(k-m) > d$, then $H^k(Ω) ⊂ C^m(\adh{Ω})$.
\end{enumerate}
\end{theorem}

The last property is the most interesting, as it tells us that regularity of a given function depends on the number of integrable derivatives and on the dimension in the space.

There are also two embeddings directly from the definition, say $H^{k+1}(Ω) ⊂ H^k(Ω)$ and $H^0(Ω) = L^2(Ω)$.

\subsection{Fractional Sobolev spaces and continuity}

Until now, we have defined Sobolev spaces with integer degree $k$. However, for the next section we will need what we call fractional Sobolev spaces. These will give us ``degrees'' of continuity: a Sobolev space of degree $4.5$ will be the space of functions that have 4 distributional derivatives in $L^p$ and for which the last derivative is ``somewhat continuous'', with that somewhat being graded by $0.5$.

This ``somewhat continuity'' actually has a formal definition: Hölder continuity.

\begin{defn}[Hölder continuity][Continuity!Hölder] Let $X,Y$ be two metric spaces and $\appl{f}{X}{Y}$ be a function between them. We say that $f$ is Hölder continuous of degree $α$ if there exists constants $C,α ∈ ℝ^+$ such that for every $x,y ∈ ℝ^d$ the following ``Hölder condition'' holds: \( \norm{f(x) - f(y)}_Y ≤ C\norm{x - y}_X^α \label{eq:Fundamentals:HolderCondition} \)

A function will be locally Hölder continuous if we can find those constants $C,α$ for a neighborhood of every point $x ∈ ℝ^d$.
\end{defn}

Although for the definition we let the degree be $α ∈ ℝ^+$, we are only interested in the case $α ∈ (0,1)$. With $α = 0$, we only have bounded variation, which is not necessarily continuous at all. With any $α > 0$, we have that $f$ is continuous. Magic happens with $α = 1$, a case for which we will need an additional definition.

\begin{defn}[Lipschitz continuity][Continuity!Lipschitz] \label{def:LipschitzCont} Let $X,Y$ be two metric spaces and  $\appl{f}{X}{Y}$ a function between them. We say that $f$ is Lipschitz continuous in $X$ if there exists a constant $K ∈ ℝ$ such that, for every $x,y ∈ X$, we have \( \frac{\norm{f(x) - f(y)}_Y}{\norm{x-y}_X} ≤ K \label{eq:Fundamentals:LipschitzCondition}  \)

As it was the case with Hölder continuity, a function will be locally Lipschitz continuous if we can find the constant $K$ for a neighborhood of every point $x ∈ X$.
\end{defn}

It is straightforward to see that Hölder continuity of degree $1$ is equivalent to Lipschitz continuity. What is more interesting is the relationship of Lipschitz continuity and derivation.

\begin{theorem}[Rademacher's Theorem] Let $\appl{f}{ℝ^m}{ℝ^n}$ be a Lipschitz continuous function. Then, it is differentiable almost everywhere (that is, the only points where it isn't differentiable have Lebesgue measure zero).
\end{theorem}

The proof is a little bit complex but uses measure theory so I'm writing it down only the part for one dimension. First of all, an important part which is the Riesz Representation Theorem, which gives us an equivalence between functionals and functions of the same Hilbert space.

\begin{theorem}[Riesz Representation Theorem] \label{thm:RieszRepresentation} Let $H$ be a Hilbert space, and let $H'$ be its topological dual space\footnote{Beware: the topological dual space is the space of all \textbf{continuous} linear functionals on $H$.}. Given $x ∈ H$ an element of the Hilbert space, we can define a continuous linear functional \[ φ_x(y) = \pesc{y,x} \;∀y ∈ H\]


The theorem states that \textbf{every} continuous linear functional has this representation as an scalar product. More broadly, the mapping $\appl{Φ}{H}{H'}$ defined as above is an isometric anti-isomorphism:
\begin{enumerate}
	\item Φ is bijective.
	\item Φ conserves norms: $\norm{Φ(x)} = \norm{x}$.
	\item Φ is antilinear: $\norm{Φ(x + y) } = Φ(x) + Φ(y)$ and $Φ(λx) = \conj{λ}Φ(x)$.
\end{enumerate}
\end{theorem}

We are also going to need the Lebesgue differentiation theorem. Just as an improved notation, we define the ``averaging integral'' by $\fint_A f \dif μ = \frac{1}{μ(A)}\int f \dif μ$ where $μ(A)$ is the measure of the set $A$.

\begin{theorem}[Lebesgue Differentiation Theorem] Let $\appl{f}{ℝ^m}{R^n}$ be a Lebesgue integrable function. Then, for almost every $x ∈ ℝ^m$ we have that \( \lim_{r \to 0} \frac{\int_{\bola_r(x)} f \dif μ}{μ(\bola_r(x))} = \fint_{\bola_r(x)} f \dif μ  = f(x) \)
\end{theorem}

This theorem gives us a nice corollary, which is the fundamental theorem of calculus for Lebesgue integrals.

\begin{corol} \label{crl:LebesgueDifferentiation} Let $f$ be a Lebesgue integrable function, and let $F(x) = \int_{-∞}^x f \dif μ$. Then $F$ is differentiable almost everywhere with $F'(x) = f(x)$.
\end{corol}

With this, we can do the proof of Rademacher's theorem. We will only need one inequality:

\begin{prop}[Hölder inequality][Inequality!Hölder] \label{prop:HolderInequality} Let $f ∈ L^p$ and $g ∈ L^q$ with $p,q$ conjugate exponents (that is, $\frac{1}{p} + \frac{1}{q} = 1$). Then, $fg ∈ L^1$ and \[ \norm{fg}_1 ≤ \norm{f}_p \norm{g}_q \]
\end{prop}

\begin{proof}[Rademacher's Theorem] What we are going to prove is that $\appl{f}{[a,b]}{ℝ}$ can be expressed as $f(x) = \int_a^b g(x) \dif x$ with $g$ differentiable. To construct this function, we first define a continuous linear functional and will then use the \nref{thm:RieszRepresentation}. For simplicity, we can assume $[a,b] = [0,1]$.

First, recall the definition of simple functions. These are functions of the form $s(x) ≝ \sum α_i \ind_{[a_i, a_{i+1}]}$. We can define a linear functional $I$ on these functions given by \[ I(s) ≝ \sum α_i \left(f(a_{i+1}) - f(a_{i})\right) \]

Now we will prove that this functional is bounded and thus continuous (linearity is trivial):
\begin{align*}
\abs{I(s)} &=\abs{\sum α_i \left(f(a_{i+1}) - f(a_{1})\right)} \\
	&≤ \sum \abs{α_i} \abs{f(a_{i+1}) - f(a_{i})} \\
	&≤ K \sum \abs{α_i} \abs{a_{i+1} - a_{i}} \quad\text{ (Lipschitz continuity)} \\
	&= K \norm{s}_1 \\
	&≤ K \norm{\ind_{(0,1]}}_2 \norm{s}_2 \quad\text{ (Hölder inequality)} \\
	&= K \norm{s}_2
\end{align*}

As the simple functions are a dense subset of $L^2$, this means that $I$ is bounded for every $L^2$ function and thus it is a continuous linear functional on $L^2([0,1])$, and the same with $L^1$. Using the \nref{thm:RieszRepresentation}, there exists a function $φ ∈ L^2([0,1])$ such that $I(ψ) = \pesc{ψ, φ}$ for every $ψ ∈ L^2([0,1])$. As $I$ is also a linear functional on $L^1$, φ must be in $L^1$ too and thus it is Lebesgue integrable.

Finally, we just choose $ψ_x = \ind_{[0,x]}$ so we have \[ I(ψ_x) = f(x) - f(0) = \int_0^x φ \dif μ \], which is the assumption of \fref{crl:LebesgueDifferentiation} and thus we have differentiability almost everywhere.
\end{proof}



Fractional sobolev spaces.

Trace allows us to enunciate boundary conditions on functions and, importantly, generalized functions. Existence of a linear operator from $H^1$ to $L^2$ functions on the boundary $∂Ω$, which is simply the restriction, and then it's bounded $\norm{γ_0v} ≤ C\norm{v}_{H^1(Ω)}$. Thing is that we can approximate by $C^∞$functions and then we define the limit. Still true if we use only a part of the boundary, as long as that part has not null measure. We can extend these ideas somehow.

Introduce the space $H^1_0(Ω)$ as the subspace of $H^1(Ω)$ with the additional property that their trace is equal to $0$.

\chapter{Elliptic problems}

The first chapter will be dedicated to the study of elliptic PDEs. Those are of the form \(
\begin{cases}
-Δu = f & \text{ in } Ω \\
u =  0 & \text{ in } ∂Ω
\end{cases} \label{eq:EllipticProblem} \) for some open domain $Ω ∈ ℝ^d$ and some function $f ∈ C^2(Ω)$.

This equation models, for example, the distribution of heat in a domain, the concentration of a chemical in a fluid at rest, an electric potential in presence of distributed charges, the deformation of a membrane...

This problem can be solved in the strong form, where we approximate the second derivative using Taylor series and then try to find the solution (finite differences method). But we can also use the weak formulation, which is used for the finite elements method.

\section{Weak formulation}

As a reminder, we start from a PDE in a strong form such as \[ -Δu = f \text{ in Ω}\]

A physical interpretation is to think of the solutions as solutions that will zero-out the forces for every possible small displacement, that is, solving \[ \int_Ω (-Δu -f) v = 0 \] for all ``virtual displacement'' $\appl{v}{Ω}{ℝ}$ that is sufficiently smooth.

In order to have an improved form of that, we use integration by parts: \begin{multline*} \int_Ω - Δu v = -\int \dv (\grad u) v = \\ = - \int_Ω \dv(\grad u · v) + \int_Ω \grad u · \grad v = - \int_{∂Ω} \underbracket{(\grad u · \vn)}_{∂_\vn u} · v + \int_Ω \grad u · \grad v \end{multline*}

We have a small problem with the integration on the boundary. But if we think of the previous physical argument, if $v$ is a virtual displacement, we should have it constrained in the boundary, so $\restr{v}{∂Ω} = 0$, the boundary term disappears and our weak formulation is \[ \int_Ω \grad u · \grad v = \int_Ω f v \]

Another way to think of this is that is $v$ is any kind of test function, it must have compact support contained in $Ω$ and thus it must be zero on the boundary.

Once we have the formulation, we can study the regularity requirements for $v$. We would like to have bounded integrals, and by applying the \nref{prop:HolderInequality} we see that we need $\norm{\grad v}_2$ and $\norm{v}_2$ to be bounded. That, together with the restriction on the boundary, means that we need $v ∈ H_0^1(Ω)$ (see the introduction for the definition of this space).

Now let's put everything together:

\begin{defn}[Weak\IS formulation of a PDE problem] Given a domain $Ω⊂ℝ^d$ and a function $f ∈ C^2(Ω)$, find $u ∈ H_0^1(Ω)$ such that \( \int_Ω \grad u · \grad v = \int_Ω f v \label{eq:WeakFormulation} \) for every $v ∈ H_0^1(Ω)$.
\end{defn}

But we can also look at these two terms of \eqref{eq:WeakFormulation} as operators on the Hilbert space \begin{align*}
\appl{a}{H_0^1(Ω)×H_0^1(Ω)&}{ℝ} & \appl{F}{H_0^1(Ω)&}{ℝ} \\
(u,v) &\longmapsto \int_Ω \grad u · \grad v  & v &\longmapsto \int_Ω f v
\end{align*} with $a$ a bilinear form and $F$ a linear form. This gives us an abstract weak formulation.

\begin{defn}[Weak\IS abstract formulation of a PDE problem] \label{def:WeakAbstractFormulation} Given a Hilbert space $V$, a bilinear form $\appl{a}{V×V}{ℝ}$ and a linear form $\appl{F}{V}{ℝ}$, find $u ∈ V$ such that \[ a(u,v) = F(v)\quad ∀ v ∈ V \]
\end{defn}

Bounded operators are already defined (check \cite{ApuntesAnalisisFunc}). For bilinear forms, two definitions:

\begin{defn}[Continous\IS bilinear form] Given $V$ a Hilbert space and $\appl{a}{V×V}{ℝ}$ a bilinear form, we say that it is continous (or bounded\footnote{As with linear forms, both notions are equivalent}) if and only if exists $M > 0$ such that \[ \abs{a(u,v)} ≤ M \norm{u}_V \norm{v}_V\] for all $u,v ∈ V$.
\end{defn}

\begin{defn}[Coercive\IS bilinear form] Given $V$ a Hilbert space and $\appl{a}{V×V}{ℝ}$ a bilinear form, we say that it is coercive if an inly if exists a constant $α > 0$ such that \[ a(u,u) ≥ α \norm{u}^2\]

As notation, α is called the coercivity coefficient
\end{defn}

WIth that, we can enunciate the Lax-Milgram lemma for the weak abstract formulation.

\begin{lemma}[Lax-Milgram\IS lemma] \label{lem:Elliptic:LaxMilgram} Given a Hilbert space $V$, a bilinear, continuous and coercive form $\appl{a}{V×V}{ℝ}$ and a linear, bounded operator $\appl{F}{V}{ℝ}$; the \nlref{def:WeakAbstractFormulation} has a unique solution $u ∈V$ such that \[ \norm{u}_V ≤ \frac{1}{α} \norm{F}_{V^*} \] with α the coercivity constant of $a$.
\end{lemma}

\subsection{Poisson problems}

It is easy to see that we can apply the \nref{lem:Elliptic:LaxMilgram} to our problem: if $v ∈ H^1_0(Ω)$, then its $L^2$-norm is bounded so \[ \abs{\int_Ω fv} ≤ \norm{f}_{L^2} \norm{v}_{L^2} ≤ \norm{f}_{L^2} \norm{v}_{H^1} < ∞ \]

Same happens with $a$: the boundedness is directly obtained from the fact that $u, v ∈ H^1_0(Ω)$. Coercivity can be a little bit more complicated. For that, we must use the Poincaré inequality TODO LINK INTRO which tells us that there is a constant $C_p > 0$ such that $\int v^2 ≤ C_p \int \abs{\grad u}^2$ for all $u ∈ H_0^1$. So, in this case we have that \[
\norm{u}^2_{H^1} = \int v^2 + \int \abs{\grad v}^2 ≤ (1+C_p^2) \int \abs{\grad u}^2
\] so our coercivity constant is $\frac{1}{1 + C_p^2}$.

This means that, applying the \nref{lem:Elliptic:LaxMilgram}, we have a unique solution $u ∈ V$ such that \[ \norm{u}_{H^1_0} ≤ (1+C_p^2) \norm{F}_{H^1_0} \]

\subsection{Poisson problems with mixed boundary conditions}

We will want to know what happens when we have mixed boundary conditions. We may have two types of boundary conditions, which we define now.

\begin{defn}[Boundary condition\IS Dirichlet] Also called essential boundary condition, is a restriction on the boundary $u = g$.
\end{defn}

\begin{defn}[Boundary condition\IS Neumann] Also called natural boundary condition, is a restriction on the normal derivative. $∂_\vn u = h$.
\end{defn}

Thus, we will study the problem \( \begin{cases}
-Δu =f & \text{in }Ω \\
u = g & \text{on }Γ_D \\
∂_\vn u = h & \text{on }Γ_N \end{cases} \) with $Γ_D ∪ Γ_N = ∂Ω$.

We can start with the weak formulation as previously \[ \int_Ω fv = \int_Ω -Δu v = \int_Ω \grad u · \grad v - \int_{∂Ω} ∂_\vn u v = \int_Ω \grad u \grad v - \int_{Γ_N} ∂_\vn v - \int_{Γ_D} ∂_n u v\]

The problem here is how to deal with the boundary term on $Γ_D$. As we did in the previous section, we can select $\restr{v}{Γ_D} = 0$. Thus, our weak formulation is \( \int_Ω \grad u · \grad v = \int_Ω f v + \int_{Γ_N} h v \label{eq:Elliptic:WeakFormulationMixedBoundary} \) with $v ∈ H^1_{Γ_D}$, where we can define \( H^1_{Γ_D} = \set{ v ∈ H^1 \tq \restr{v}{Γ_D} = 0 } \)

Our solution should however live in $V_g = \set{v ∈ H^1 \tq \restr{v}{Γ_D} = g}$, which is not linear but only an affine subspace.

If we have $g = 0$, we still can use a \nlref{def:WeakAbstractFormulation} with $F(v) = \int fv + \int_{Γ_N} hv$ to find a solution $u ∈ H^1_{Γ_D} = V_0$. With $g ≠ 0$ things become a little bit more difficult.

In that case, we need to find a solution $u ∈ V_g$. We will then separate the problem in two, writing $u = u_0 + G$ with $G ∈ H^1$ such that $\restr{G}{Γ_D} = g$, and then solving for $u_0 ∈ V_0$ as before. The question is whether this $G$ can be constructed. Luckily, the theory will say that $∀g ∈ H^{\sfrac{1}{2}}(Γ_D)$ there will be a function $G ∈ H^1$ such that $\restr{G}{Γ_D} = g$ and $\norm{G}_{H^1} ≤ γ \norm{g}_{H^{\sfrac{1}{2}}(Γ_D)}$.

\section{Calculus of variations}

We can try to study other approach to this problem with an example, which is the deformation $u$ of a membrane $Ω$ under a certain force $f$. In that case, we try to find a solution that minimizes the elastic energy $E = \int_Ω \frac{1}{2} κ \norm{\grad u}^2$. Supposing $κ = 1$, our energy functional to minimize is \( J(u) = \frac{1}{2} \int_Ω \norm{\grad u}^2 - \int_Ω f u - \int_{Γ_N} h u \label{eq:Elliptic:EnergyFunctional} \) so our solution should be \[ u = \argmin_{\substack{v ∈ H^1(Ω) \\ v = \restr{g}{Γ_D}}}  J(v) \] where we search for the function in $H^1(Ω)$ because we need for the gradient to be square integrable. We need also $f ∈ L^2$, and $h ∈ H^{-\sfrac{1}{2}}$ (the topological dual space of $H^{\sfrac{1}{2}}$) to be able to integrate that last term.

So, how do we do this? If the argument was real, we could find the point with gradient $0$ (all directional derivatives are null). But the functional $J$ from \eqref{eq:Elliptic:EnergyFunctional} is an application $\appl{J}{H^1}{ℝ}$, so we need something different. However, we can still translate the concept of ``gradient $0$''. If $J$ were a real variable function, we could do \[ \grad J (u) = 0 \iff \grad J(u) · \vA = 0\; ∀\vA ∈ ℝ^N \iff \lim_{ε \to 0} \frac{J(u + ε\vA) - J(u)}{ε} = 0 \]

That last notion is the one we can translate to the functional case. If we consider $u$ to be the equilibrum, we can study any variation of $u ∈ V_g$\footnote{Remember from the previous section that $V_g = \set{v ∈ H^1 \tq \restr{v}{Γ_D} = g}$.} such that $u + ε v ∈ V_g$ (that is, respecting the boundary conditions), with $v ∈ V_g$ forcibly.

Knowing this, we can try to calculate the ``derivative'', where some linear terms will be canceled but we will have to deal with the quadratic ones:
\begin{align*}
\Dif_v J(u) &= \lim_{ε \to 0} \frac{J(u + εv) - J(u)}{ε} = \\
	&= \lim_{ε \to 0} \frac{1}{ε} \left[ \frac{1}{2} \int_Ω \grad(u+εv)· \grad(u + εv) - \int_Ωf(u + εv) - \int_{Γ_N} h · (u + εv)\right. \\
	&\qquad \left.- \frac{1}{2}\int_Ω \grad u \grad u + \int_Ω fu + \int_{Γ_N} h u \right] = \\
	&= \lim_{ε \to 0}\frac{1}{ε} \left[ \frac{1}{2}\left( \int_Ω \grad(u+εv) \grad (u + εv) - \grad u \grad u \right) - ε \int_Ω fv - ε \int_{Γ_N} h v \right] = \\
	&= \lim_{ε \to 0}\frac{1}{ε} \left[ \frac{1}{2} \left( \grad u \grad u + ε \grad u \grad v + ε \grad v \grad u + ε^2 \grad v \grad u - \grad u \grad u\right) - ε \int_Ω fv - ε \int_{Γ_N} h v \right] = \\
	&= \int_Ω \grad u \grad v - \int_Ω fv - \int_{Γ_N} h v
\end{align*} which is the same weak formulation of the problem we had previously in \eqref{eq:Elliptic:WeakFormulationMixedBoundary}.

\subsection{Regularity of the solution}

We may have proved that the solution exists, but we will also be interested in knowing the regularity of the function in order to be able to prove rates of convergence, for example. That will be given in the following theorem.

\begin{theorem}[Shift theorem] Consider the PDE problem \[
-Δu =f \qquad \text{in }Ω \] with $f ∈ H^m(Ω)$, $Ω$ a smooth domain ($∂Ω ∈ C^{m+2}$, that is, we can parametrize the boundary as a $C^{m+2}$ manifold) and with the following restrictions depending on the boundary conditions:
\begin{itemize}
	\item Full Dirichlet conditions $\restr{u}{Γ_D} = g ∈ H^{m + \sfrac{3}{2}}(Ω)$.
	\item Full Neumann conditions $∂_\vn u = h ∈ H^{m + \sfrac{1}{2}}(Ω)$.
\end{itemize}

Under those conditions, $u ∈ H^{m+2}(Ω)$.
\end{theorem}

\subsubsection{Corner singularities}

\begin{wrapfigure}{L}{0.3\textwidth}
\centering
\inputtikz{CornerSingularity}
\caption{Corner singularity in a domain.}
\label{fig:Elliptic:CornerSingularity}
\end{wrapfigure}

One could try to see what happens if we have corner singularities, for example, in a square domain. In a set inside of the square we will have perfect smoothness, but the corners may present problems.

Suppose we want to solve $- Δ u = 0$ in a corner such as \fref{fig:Elliptic:CornerSingularity}. In that case, we will work in polar coordinates and the basis of our solution will be \[φ_k(r,θ) = r^{\frac{kπ}{ω}} \sin \frac{kπθ}{ω} \]

The worst case would be $k = 1$, which would leave us in a case of $u ∈ H^s$ with $s < 1 + \sfrac{π}{ω}$. If $ω > π$ (the case of the square), we have $s < 2$, that is, we only have $H^1$ regularity.

We would have the same situation with Neumann conditions. However, mixed boundary conditions (Neumann on one side, Dirichlet on the other) are more problematic: the solutions are \[ φ_k(r,θ) = r^\frac{(k + \sfrac{1}{2})π}{ω} \sin \frac{(k + \sfrac{1}{2})πθ}{ω} \] and, in the worst case ($k = 0$) we have singularities even in the flat case ($ω = π$) which was not a problem in full Neumann or Dirichlet conditions.

\section{Example problems}

\subsection{Advection-diffusion-reaction}

A more complex form of elliptic problems are advection-diffusion-reaction, where the differential operator is \[ Lu = - \sum_{i,j=1}^d \dpd{}{x_i} \left(a_{ij}\dpd{u}{x_j}\right) + \sum_{i=1}^d b_i \dpd{u}{x_i} + c u = - \dv (A(x) \grad u) + \vb(x) \grad u + c u\] with $A(x) ∈ ℝ^{d×d}, \vb(x) ∈ ℝ^d, c ∈ ℝ$. Our problem, as usual, is $Lu = f$.

These three terms model respectively diffusion, with $A$ being the matrix of coefficients that depend on the axis (the material is not uniform), the transport term along the vector field, and the reaction (for example, a chemical that reacts and its concentration decreases)

In order for the problem to remain elliptic, we require $A(x)$ to be positive definite for all $x ∈ ℝ^d$.

As in previous cases, we can have Dirichlet boundary conditions $\restr{u}{Γ_D} = g$ and Neumann boundary conditions (which are a little bit more complicated): \[ A \grad u · \vn - (b \vn) u = h \quad\text{ on } Γ_N \]

We will want to do a weak formulation where the boundary terms of the problem appear naturally. As always, we multiply by a test function and integrate by parts the divergence and possibly the $b \grad u$ depending on the boundary conditions. Without doing the computations, the weak formulation will end up being \( \int_Ω A \grad u \grad v + b \grad u v + c u v = \int_Ω fv + \int_{Γ_N} h v \label{eq:Elliptic:ADRProblemWeak} \) with $u ∈ H^1$, $\restr{u}{Γ_D} = g$.

This has the same structure as in previous cases, so we are in the conditions of the \nref{lem:Elliptic:LaxMilgram} and there exists a unique solution. Coercivity would need a little bit of work, and may present problems with mixed boundary conditions. Enforcing Neumann conditions on boundaries with incoming flow may cause problems with coercivity.

\subsection{Linear elasticity}

\begin{figure}[hbtp]
\inputtikz{ElasticDeformation}
\caption{Elastic deformation of a something.}
\label{fig:Elliptic:ElasticDeformation}
\end{figure}

In this problem, we start with an undeformed configuration $Ω ⊂ ℝ^d$, and we want to study the displacement $\appl{\vu}{Ω}{ℝ^d}$. The involved terms are the strain measure \[ ε (\vu) = \frac{\grad \vu + \trans{(\grad \vu)}}{2}\], the stress tensor $σ = σ(ε)$ given by \[ σ_{ij} = \sum_{k,l=1}^d c_{ijkl} ε_{kl} \], which usually can be expressed as \[ σ(ε) = 2με + λ\tr(ε) I \] with $μ,λ$ the Lamé constants.

With all of this, our balance equation is \( \begin{cases} - \dv σ(ε(\vu)) = \vec{f} & \text{in } Ω \\
\vu = \vec{g} & \text{on } Γ_D \\
σ(u) · \vn = \vd & \text{on } Γ_N \end{cases} \)

We may want to write now our weak formulation and integrate by parts, caring a little bit about what is a tensor and what is a vector
\begin{align*}
0
	&= \int_Ω \left[- \dv (σ(ε(\vu))) - \vec{f} \right] · \vv = \\
	&= \int_Ω σ(ε(\vu)) \grad \vv - \int_{∂Ω} (σ · \vn) · \vv - \int_Ω \vec{f} \vv \\
\int_Ω σ(ε(\vu)) \colon \vv &= \int_Ω \vec{f}\vv + \int_{Γ_N} \vd \vv
\end{align*}

I'm a little bit lost but \[ \int_Ω - \dv (σ) \vv = \int_Ω - \sum_i \sum_j ∂_j σ_{ij}v_i = \sum_{ij} \int_Ω σ_{ij}∂_jv_i - \int_{∂Ω}σ_{ij}n_j v_i \]

We can rewrite the first term because \begin{align*}
\int_Ω σ (ε(\vu)) \colon \vv
	&= \int_Ω σ(ε(\vu))\colon ε(\vv) = \\
	&= \int_Ω(2με(\vu) + λ\tr(ε(\vu))I) \colon ε(\vv) = \\
	&= \int_Ω 2με(\vu) \colon ε(\vv) + λ\tr(ε(\vu))I \colon ε(\vv) = \\
	&= \int_Ω 2μ \frac{\grad \vu + \trans{(\grad \vu)}}{2} \colon \frac{\grad \vv + \trans{(\grad \vv)}}{2} + λ\dv \vu \dv \vv = \\
	&= a(\vu, \vv)
\end{align*}

So we have a bilinear form again and whatever.

\section{Galerkin approximation}

The idea of the Galerkin approximation is, once we have a \nref{def:WeakAbstractFormulation} $a(u,v) = F(v)$ for all $v ∈ V$, limit the problem to a finite-dimensional Hilbert space on which we can solve computationally the problem.

Formally, we introduce a sequence of finite dimensional subspaces $V_h ⊂ V$ with $\dim V_h = N_h$ and with a certain approximability property: that for every $v ∈ V$ we can approximate it as well as we want: \[ \lim_{h\to 0} \inf_{v_h ∈ V_h} \norm{v-v_h}_V = 0 \]

\begin{defn}[Generalized Galerkin Formulation][Galerkin formulation!Generalized] \label{def:GalerkinFormulationGen} Given a \nref{def:WeakAbstractFormulation}, we can reformulate it as finding $u_h ∈ V_h$ such that \[ a_h(u_h, v_h) = F_h(v_h) \quad ∀v_h ∈ V_h \] with the requirement that $a_h \convs[][h] 0$, $F_h \convs[][h][0] 0$ for some definition of convergence of functionals that we will see latters.
\end{defn}

However, the restriction that $V_h ⊂ V$ can be sometimes too strict. A \concept[Galerkin formulation!Non-conforming]{Non-conforming Galerkin Formulation} is one with $V_h\nsubseteq V$. That takes it to the Petrov-Galerkin approximation in which we allow different spaces for the solution and for the test functions.

\begin{defn}[Petrov-Galerkin approximation] Find $u_h ∈V_h$ such that \[ a_h(u_h, v_h) = F(v_h) \quad ∀v_h ∈ W_h \]
\end{defn}

Now to the interesting part: are these problems well-posed? Do they have unique solutions? And, more importantly, do these solutions converge to the actual solution? We will discuss it now.

\begin{prop} The formulation of a PDE problem as a \nref{def:GalerkinFormulationGen} has an unique solution.
\end{prop}

\begin{proof}
The idea here will be to apply the \nref{lem:Elliptic:LaxMilgram}. We know that $(V_h, \norm{·}_V)$ is a Hilbert space, $a(·,·)$ is continuous in $V_h$ because it is continuous in $V$, so $\abs{a(u,v)} ≤ M \norm{u}_V \norm{v}_V\; ∀v∈V_h ⊂ V$, and is coercive with at least the same coefficient because of the same reason. Same happens for $F$, so we are in the conditions of the lemma and thus the problem has an unique solution.
\end{proof}

Now, on to the quality of the approximation. If our bilinear form is symmetric, we can use Galerking orthogonality:

\begin{defn}[Galerkin orthogonality][Orthogonality!Galerkin] Given a bilinear form $a$, we say that a solution $u$ and its approximation $u_h$ fulfill the Galerkin orthogonality condition if and only if \[ a(u - u_h, v_h)= 0 \quad v_h ∈ V_h \]

Given that a coercive symmetric bilinear form defines an inner product, Galerkin orthogonality is equivalent to saying that $u - u_h \perp V_h$.
\end{defn}

The usefulness of this property is the fact that, if $u - u_h$ is orthogonal to $V_h$, then we have an \concept{Optimatility property} given by \( \norm{u - u_h}_a ≤ \inf_{v_h ∈ V_h} \norm{u - v_h}_a \label{eq:Elliptic:Optimality} \) with $\norm{·}_a = \sqrt{a(·,·)}$ the norm induced by the inner product $a(·,·)$.

For the general case, we have the following lemma for the optimality result

\begin{lemma}[Ceà's Lemma] Given a \nref{def:GalerkinFormulationGen}, the following optimality result holds: \[ \norm{u - u_h}_V ≤ \frac{M}{α} \inf_{v_h ∈ V_h} \norm{u - v_h}_V \] for $M ∈ R^+$ and $α$ the coercion coefficient of $a$.
\end{lemma}

\begin{proof} Using the coercion property, we know that \[ \norm{u - u_h}^2_V ≤ \frac{1}{α} a(u - u_h, u -u_h)\]

Adding and substracting $v_h ∈ V_h$, we can continue so \begin{align*}
\norm{u - u_h}^2_V &≤ \frac{1}{α} a(u - u_h, u -u_h + v_h - v_h) \\
	&= \frac{1}{α} a(u - u_h) + \frac{1}{α} \underbracket{a(u - u_h, v_h - u_h)}_{=0\;\text{(Galerkin Orthogonality)}} \\
\norm{u - u_h}^2_V &≤ \frac{M}{α} \norm{u - u_h}_V\norm{u - v_h}_V \\
\norm{u - u_h}_V &≤ \frac{M}{α} \inf_{v_h ∈ V_h} \norm{u - v_h}_V
\end{align*}
\end{proof}

Knowing that there are optimal and unique solutions, we can go on to an algebraic formulation. Let $u_h = \sum_j u_j φ_j$, $v_h = \sum_i v_i φ_i$ with $\set{φ_i}_{i=1}^{N_h}$ a basis of $V_h$. Then, the problem becomes
\begin{align*}
a(u_h,v_h) &= F(v_h) \\
a\left(\sum_j u_j φ_j, \sum_i v_i φ_i\right) &= F\left(\sum_i v_i φ_i\right) \\
\sum_{j,i} u_j v_i \underbracket{a(φ_j, φ_i)}_{A_{ij}} &= \sum v_i \underbracket{F(φ_i)}_{F_i}
\end{align*}

We can write this in a matrix form. If $\vu = (u_1, \dotsc, u_{N_h})$ and $\vv = (v_1, \dotsc, v_{N_h})$ then the equation becomes \begin{align*}
\trans{\vv}A \vu &= \trans{\vv} \vf \quad ∀\vv ∈ ℝ^{N_h} \\
A \vu &= \vf
\end{align*} which is a system of linear equations that can be solved in a purely algebraic manner. There are some nice properties of the matrix $A$ that come from the problem statement. It is positive definite because $a$ is positive.

\subsection{Finite element spaces}

\begin{defn}[Finite element space] A finite element space is a space of piecewise polynomial functions over a partition of a domain $Ω ∈ ℝ^N$ into non-overlapping polyhedra, called a mesh.
\end{defn}

\begin{defn}[Polyhedral mesh] A polyhedral mesh $τ_h$ on $Ω ∈ ℝ^N$ is the union of a finite number of polyhedra $K_j$, $j = 1, \dotsc, N_k$ such that $\bigcup_{j=1}^{N_k} \adh{K_j} = \adh{Ω}$ and $\intr{K}_j ∩ \intr{K}_i = ∅$ if $i ≠ j$.
\end{defn}

Usually, in 2D the polyhedrae used are either triangles or squares. In 3D, we have tetrahedron, cubes or rectangular pyramids.

\begin{defn}[Geometrical conformal mesh] A geometrical conformal mesh is a mesh for which if $\adh{K_i} ∩ \adh{K_j} ≠ ∅$ then the intersection is either a common vertex or a common edge or face. That means that half-edge intersections are not allowed in this case.
\end{defn}

A mesh can be defined by certain parameters.

\begin{itemize}
	\item \concept{Element diameter} of an element $K ∈ τ_h$ as $h_K = \max_{x,y∈K} \abs{x-y}$.
	\item \concept{Element inner diameter} $ρ_K$  as the diameter of the largest ball in $K$.
	\item \concept{Mesh size} $h = \max_{K∈τ_h} h_K$ which gives an idea of the size of the largest element of the mesh.
	\item \concept{Aspect ratio} $γ_K = \sfrac{h_K}{ρ_K}$. A high aspect ratio indicates elongated elements, while a ratio near to one indicates more regular elements.
	\item \concept{Mesh minimum size} $h_{min} = \min_{K ∈ τ_k} h_K$.
\end{itemize}

\begin{defn}[Regular family of meshes] A family of meshes $\set{τ_h}_{h \to 0}$ is said to be regular if the maximum aspect ratio is bounded by some constant $γ$ for all $h$: $\max_{K ∈ τ_h} γ_K ≤ γ ∈ ℝ$. In other words, force that for all $h > 0$ and $∀K ∈ τ_h$ $h_K ≤ γρ_K$.
\end{defn}

\begin{defn}[Quasi-uniform family of meshes] A family of meshes is quasi-uniform if it is regular and $\sfrac{h_{min}}{h} ≥ δ$.
\end{defn}

\appendix

\chapter{Exercises}
\input{tex/NumericalApproximationofPDEs_Exerc.tex}

\backmatter
\bibliography{../EPFLNotes.bib}

\printindex
\end{document}
