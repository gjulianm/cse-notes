\documentclass[palatino]{epflnotes}

\title{Computer simulation of physical systems}
\author{Guillermo Julián Moreno}
\date{16/17 - Fall semester}

% Additional packages
\usepackage{tikztools}
% --------------------

\begin{document}
\frontmatter
\pagestyle{plain}
\maketitle

\tableofcontents
\mainmatter
% Content

\chapter{Preliminaries}

\textit{Trigger warning: Mathematics explained by a physics professor.}

\section{Differential equations}

We are interested in a general differential equation of the form \[ \od{y}{x} = f(x,y) \], that is, first order equations. We are not interested in second order problems, although we can always downgrade. For example, given a momentum equation such as \[ m \od[2]{z}{t} = F(z) \], we can simplify to two first-order equation such as \[ \od{z}{t} = \frac{p(t)}{m} \qquad \od{p}{t} = F(z) \]

All this boils down that we're are going to have initial value problems for $\vy(0) = y_0$, and then we will want to get how does $\vy$ evolve with time.

In order to solve these kinds of equations numerically, we will have several points? steps?. The first one is \textbf{discretization}, introducing evaluations at a finite number of points with step $h$\footnote{That is, points given as $x_n = x_0 + n·h$.}. The issue is the size of the step, which depends on the application being done.

The second point is a \textbf{recursion formula}: that is, how to get the $n$-th value $y_n$ as a function of the previous values $y_{n-1}, y_{n-2}, \dotsc$.

The simplest formula is called the Euler method.

\begin{prop}[Euler method] \label{prop:EulerMethod} Let $y(x)$ be a ¿? function with the differential equation $\iod{y}{x} = f(x,y)$ with $f$ being another ¿? function. Let $y(x_n) = y_n$ be a value of the equation. Then we can obtain the next value by the given formula: \( y_{n+1} = y_n + h · f(x_n, y_n) + \mathcal{O}(h^2) \)
\end{prop}

However, this is a simple method and not very accurate, as it has a low convergence rate. Another method is based on Taylor expansion of again functions in some absolute random space.

\begin{prop}[Taylor expansion method] In the same conditions (that is, no conditions) of \fref{prop:EulerMethod}, we can use a method of higher-order accuracy: \( y_{n+1} = y_n + hf(x_n, y_n) + \frac{1}{2}h^2 \left(\dpd{f}{x}(x_n, y_n) + f (x_n, y_n) \dpd{f}{y}(x_n, y_n\right) + \mathcal{O}(h^3) \label{eq:TaylorExpansion} \)
\end{prop}

We can, however, achieve the same accuracy with a three-point method.

\begin{prop}[Symmetric differences method] In the same no conditions previously not explained, we can approximate by \( y_{n+1} = y_{n-1} + 2hf(x_n, y_n) + \mathcal{O}(h^3)\)
\end{prop}

The disadvantage is that here we need to be able to differentiate $f$ with respect to $x$ and $y$, something that will not always be easy.

Runge-Kutta. Advantages: accuracy 2nd order,  no easy differentiability required (unlike Taylor), can start right away (unlike multistep), no linearity $f$ in $y$ (implicit linearity)
 and can change step size.

\subsection{Stability}

\begin{figure}[hbtp]
\inputtikz{ExponentialApprox}
\caption{A numerical approximation of the ODE $y' = -y$ can suffer from stability problems when the solution goes to 0.}
\label{fig:ExponentialApproxProblem}
\end{figure}

One interesting aspect to study is the stability of the methods. For example, let's consider the problem \begin{align*} y' &= -y \\ y(0) &= 1 \end{align*}

This problem has an exact solution given by $y = e^{-x}$.

\chapter{Molecular dynamics}

\section{Classical molecular dynamics}

One of the main purposes when doing simulations of molecular dynamics is to find equilibrum properties of some quantity that depends on the position of the particles and their momemtum. We might also want to calculate time correlation functions, seeing how does a particle evolve in time.

The first step if the definition of the variables and their interactions, which is done by the definition of the hamiltonian. It is also necessary to define the initial configuration. There will finally issues with boundary conditions: we don't want to see physical effects at boundaries so there are several possibilities.

We will be also interested in the cutoff radius $r_C$, the limit until which we suppose forces can act.

However, we will find various issues: the quality of the interactions, the size of the system, the time, accuracy and statistical noise limit the size of the system we can simulate.

Depending on what we want, we can use several methods. If we want to take ensemble averages, we don't care about the exactness of trajectories. However, we will care about that if the want to get correlation functions.

Considerations for algorithms:

\begin{enumerate}
	\item One force evaluation per time step.
	\item Accuracy $o(Δt^4)$: we want to take into account the velocity, the force and the change in acceleration to account for changes in stability.
\end{enumerate}

\subsection{Verlet-Störmer algorithm}

If we start from \[ r(t + Δt) = r(t) + v(t) Δt + \frac{f(t)}{2m}Δt^2 + \frac{Δt^3}{3!}R''' + o(Δt^4) \] we can add $r(t+Δt) + r(t-Δt)$ and then we have the equation \[ r(t + Δt) = 2r(t) - r(t - Δt) + \frac{f(t)}{m}Δt^2 + o(Δt^4) \] which is the position-Verlet and only requires one evaluation of the force. This does not require any evaluation of the velocity.

However, in order to be able to verify the conservation of energy in a system we need the velocity (kinetic energy) which is easily calculated as \[ v(t) = \frac{r(t + Δt) - r(t-Δt)}{2Δt} + o(Δt^2) \]

In order to have more accurate velocity, we have to have Newton equation $m\ddot{r} = f$ which deriving again is $\dddot{r} = \dot{f}{m}$ which can be approximated by $\frac{f(t+Δt) - f(t-Δt)}{2Δtm} + o(Δt^2)$ and the final formula looks like \[ v(t) = \frac{r(t + Δt) - r(t-Δt)}{2Δt} + \frac{Δt}{12}\left[f(t+Δt)-f(t - Δt)\right] + o(Δt^6)\] which however is not especially good neither.

The disadvantage of this algorithm is the factor $2r(t) - r(t - Δt)$, which is a problem if these two quantities and large but their difference is small, as the accuracy of floating point numbers can fail here.

\subsection{Leap-frog Verlet}

To fix the previous issues, we introduce a half-step: \begin{align*}
r(t + Δt) &= r(t) + Δt · v(t + \sfrac{Δt}{2}) \\
v(t + Δt) &= v(t - \sfrac{Δt}{2}) + Δt· \frac{f(t)}{m}
\end{align*}

This algorithm is equivalent to the previous one, I'm not going to copy the steps.

The advantage is that there is no loss of accuracy due to differences between large numbers, but we don't know velocities and posiitons at the same moment.

\subsection{Velocity Verlet}

The most convenient algorithm for the classical method without the problem of the previous two is \begin{align*}
r(t + Δt) &= r(t) + Δt v(t) + \frac{Δt^2 F(t)}{2m} \\
v(t + Δt) &= v(t) + \frac{Δt}{2m}\left[F(t) + F(t + Δt)\right]
\end{align*}

Again, this is equivalent to the previous ones.

The nice thing about this algorithm is that it gives conservation of energy (supposing one doesn't make roundoff errors).

\subsubsection{Stability of Verlet algorithms}

We are going to IDK with an harmonic oscillator: \[ \ddot{x}(t) = -ω_0^2 x(t)\] which has the solution $x(t) = e^{\imath ω t}$ with $ω$ related to the discretization $h$ as \[ 2 - 2\cos ω h = h^2 ω_0^2\]

Here we can see that $h^2 ω_0^2 > 4$, we don't have any real solution for $ω$ and thus the solution diverges immediately (exponential order).

On the other hand, if $ωh \ll 1$ we can do computations and see that $ω ≠ ω_0$ if $h ≠ 0$, so we see something that is not exactly the real solution.

\subsection{Gear algorithm}

Given a force $\ddot{x}(t) = f(x)$, we define the vector \[ \vy_n = \begin{pmatrix} x_n & Δt \dot{x}_n & \frac{Δt^2}{2} \ddot{x}_n & \frac{1}{6} Δt^3 \dddot{x}_n \end{pmatrix}^T \]

The first step is a predictor based on Euler/Taylor, so we can predict the next position based on \[ \vy_{n+1}^P = \underbracket{\begin{pmatrix} 1 & 1 & 1 & 1 \\ 0 & 1 & 2 & 3 \\ 0 & 0 & 1 & 3 \\ 0 & 0 & 0 & 1 \end{pmatrix}}_A \vy_n \]

Then we calculate the force $f(x_{n+1}^P)$ (the position is the first component of $\vy^P$), and then the final step is the correction \[ \vy_{n+1} = \vy_{n+1}^p + \va \frac{Δt^2}{2} \left[f(x_{n+1}^P) - \ddot{x}_{n+1}^P \right] \] with $\va = \begin{pmatrix} \sfrac{1}{6} &\sfrac{5}{6} & 1 & \sfrac{1}{3}\end{pmatrix}^T$.

The criteria for fixing the values are accuracy, stability and of course the fact that the force doesn't depend on time.

Again, we can test the precision of this algorithm on a harmonic oscillator.  We want to see the maximum error, the noise in the total energy and the drift (how much energy are we losing).

Magical transparency, Gear is best at small steps, become comparable to Verlet at higher values.

\section{Simulation as a computer experiment - Rahman's simulation}

Box something, periodic boundary conditions. NVE (constant temperature) and the lennard-Jones potential \[ U = \sum_{i < j} u_{ij} = \sum_{i < j} u^{ij}\norm{r_i - r_j} \] with \[ u^{ij}(r) = 4ε \left(\frac{σ^2}{r^{12}} - \frac{σ^6}{r^6} \right)\]

Computations of atomic forces. Instantaneous temperature: equipartition theorem. Radial distribution function.

\subsection{Diffusion coefficient in molecular dynamics}

But we can also study things that are not static. For example, we might want to study the \textbf{diffusion coefficient}, which is related to Fick's law explaining macroscopic behaviour of liquids: \( \vec{J} = -D \grad c \) where $\vec{J}$ is the current in the material, $c$ is the concentration and $D$ is the diffusion coefficient.

We also have the continuity relation, which assures us that the material does not disappear: \( \dpd{c}{t}(\vr, t) + \dv \vec{J} (\vr, t) = 0\)

Combining these equations gets us the diffusion equation: \( \dpd{}{t} c(\vr, t) - D Δc(\vr,t) = 0 \) which for initial conditions\footnote{HAASIODJAHSDA DELTA IS NOT A FUNCTION!} $c(\vr, 0) = δ_{\vr_0}$ has solution \[ c(\vr, 0) = \frac{1}{\left(4πDt\right)^{\sfrac{3}{2}}} e^{-\frac{\norm{\vr}^2}{4Dt}} \]

We might want to study the average of the squared norm of $\vr$: \[ \int_{ℝ^N} c(\vr, t) \norm{\vr}^2 \dif \vr = \dotsb = 6 Dt\] which gives us a way of obtaining the diffusion coefficient experimentally.

What happens, however, if the initial condition is not a delta \textit{distribution}? Well, the solution is the convolution of the initial conditions $f(\vr) ∈ L^1(ℝ^N)$ with $\norm{f}_1 = 1$  with the essential solution (Gaussian) given by \[ \Phi(\vr,t)=\frac{1}{\sqrt{4\pi Dt}}\exp\left(-\frac{\vr^2}{4Dt}\right) \] so our equation becomes \begin{align*}
\int_{ℝ^N} c(\vr, t) \norm{\vr}^2 \dif \vr &= \int_{ℝ^N} (f \ast Φ) \norm{\vr}^2 \dif \vr = \\
	&= \int_{ℝ^N} \left(\int_{ℝ^N} f(\vx) Φ(\vr - \vx, t) \dif \vx \right) \norm{\vr}^2 \dif \vr = \\
	&= \int_{ℝ^N} f(\vx) \left(\int_{ℝ^N} Φ(\vr - \vx, t) \norm{\vr}^2 \vr \right) \dif \vx = \\
	&= 6Dt · \int_{ℝ^N} f(\vx) \dif x = 6Dt
\end{align*} which agains tells us that the evolution of the second moment is proportional to $t$ and the diffusion coefficient.

In a simulation, we would calculate the diffusion coefficient as \[ D = \lim_{t \to ∞} \frac{1}{6t} \pesc{\norm{Δ\vr(t)}^2} \] with \[ \pesc{\norm{Δ\vr(t)}^2} = \frac{1}{N} \sum_{i=1}^N \norm{\vr_i(t) - \vr_i(t_0)}^2\] for $t_0$ the initial time of the simulation.

A simulation of Rahman's experiment reveals that in short time scales (picoseconds) Fick's law already applies at the atomic level.

\subsubsection{Green-Kubo relation for the diffusion coefficient}

Another way to obtain the diffusion coefficient is something that I did not listen to but the formulas are \[ Δ\vr_x(t) = \int_0^t \dif t' \vv_x(t') \] and \begin{align*}
\pesc{\abs{Δ\vr_x(t)}^2} &= \pesc{\left(\int_0^t \dif t' \vv_x(t')\right)^2 } = \\
&= \dotsb =  \\
&= 2 \pesc{\int_0^t \dif t' \int_0^{t'} \dif t'' \vec{v}_x(t') \vv_x(t'') }
\end{align*}

And now \[ D = \lim_{t\to ∞} \frac{1}{2} \dpd{}{t} \pesc{\abs{Δ\vr_x}^2}\] beacuse the material is isotropic or something like that and at equilibrum and after some horrible calculations we have that \[ D = \int_0^{∞} \pesc{\vv_x(τ) \vv_x(0)} \dif τ \]

\section{Constant temperature simulations - Sampling in the canonical ensemble}

Although in a setting with an infinite number of particles temperature can remain constant, in the canonical ensemble (CE) there are specific fluctuations related to the fact that there is only a finite number of particles. So $T_i \propto \sum_{I} p_I^2$ and then \begin{multline*} \frac{\pesc{(ΔT)^2}}{T^2} = \frac{\pesc{(T_i - \pesc{T_i})^2}}{\pesc{T_i}^2} = \frac{\pesc{\left(\sum_I p_I^2\right)^2} - \left(N\pesc{p}\right)^2}{\left(N\pesc{p}\right)^2} = \frac{\pesc{\sum_{I,J}p_I^2p_J^2} - \left(N\pesc{p}\right)^2}{\left(N\pesc{p}\right)^2} = \\ = \frac{N\pesc{p^4} + N(N-1)\pesc{p^2}\pesc{p^2} - \left(N\pesc{p}\right)^2}{\left(N\pesc{p}\right)^2} = \frac{1}{N} \frac{\pesc{p^4} - \pesc{p^2}^2}{\pesc{p^2}^2} = \frac{2}{3N}\end{multline*}

We have used here the distribution of momentum Maxwell-Boltzmann given by \[ \mathcal{P}(p) = \left(\frac{β}{2πm}\right)^{\sfrac{3}{2}} e^{-\frac{βp^2}{2m}} \] with $β = \frac{1}{kT}$, so the moments can be calculated \begin{align*}
\pesc{p^2} &= \frac{3m}{β} &
\pesc{p^4} &= 15 \frac{m^2}{β^2}
\end{align*}

In the microcanonic ensemble, Lebowitz et al. found that the fluctuations are \[ \frac{\pesc{(ΔT)^2}}{T^2} = \frac{2}{3N} \left(1 - \frac{3k}{c_v} \right)\]

The method to solve this problem goes by the name of \textbf{Extended Lagrangian} or \textbf{Nosé-Hoover thermostat}, based on two papers.

The quantity calculated depends on static quantities, position and momentum\footnote{Don't ask me what on earth does $\dif p_i^N$ mean.}:
\( \bar{A} = \pesc{A\left(\set{r_i}, \set{p_i}\right)} = \frac{\int A(r_i', p_i') e^{-βH(p_i', r_i')}\dif p_i^N \dif r_i^N}{\int e^{-βH(p_i',r_i')} \dif p_i^{'N} \dif r_i^{'N}} \label{eq:QuantityThermostat} \) and under the conditions og ergodicity we have that \[ \bar{A} = \lim_{τ \to ∞} \frac{1}{τ} \int_0^τ \det A[r_i(t), p_i(t)] \dif t\]

The idea of the extended lagrangian of Nosé is something. Scaling of velocities so $\dot{r}_k \to s \dot{r}_i$ and $s$ scales time. The Lagrangian looks like this \[ \mathcal{L}^{\text{Nosé}}(\set{r_i}, \set{p_i}, s, \dot{s}) = \sum_{i = 1}^N \frac{m_i}{2} s^2 \dot{r}_i^2 - U(\set{r_i}) + \frac{Q}{2}\dot{s}^2 - V(s) \] with $U$ the potential energy of the system, $\frac{Q}{2}\dot{s}$ the kinetic energy of $s$ ¿? and $V$ is a potential energy that satisfies the correct conditions: \[ V(s) = \frac{g}{β} \log s \]

The advantage of this chaos is that the quantities $r_i, p_i$ of \eqref{eq:QuantityThermostat} do not change. With the Lagrangian we can get the momentum so \[ p_i = \dpd{\mathcal{L}}{\dot{r}_i} = m_i s^2 \dot{r}_i\qquad p_s = \dpd{\mathcal{L}}{\dot{s}} = Q \dot{s} \]

And somehow the Hamiltonian is \[ \mathcal{H}_{\text{Nosé}} = \sum_{i=1}^N \frac{p_i^2}{2m_is^2} + U(\set{r_i}) + \frac{p_s^2}{Q} + \frac{g}{β} \log s\]

As we don't have dependence on time in that Hamiltonian, that quantity is conserved.

Let's look at the microcanonic partition function, which is a mathematical abortion: \[ Q_{\text{Nosé}} = \int \dif p_s \dif s \dif p^N \dif r^N δ\left(\mathcal{H}_{\text{Nosé}}(p^N, r^N, s, p_s) - E\right) \] and I feel like my degree in mathematics is slipping out of my hands. Holy crap that's horrible. Of couse he also called the δ a function.

So he does a little bit more of mathematical heresy and says that $p' = \sfrac{p}{s}$ with $p'$ a physical variable and does a change of variable in and integral that IS NOT FUCKING WELL DEFINED and now he says this aberration \[ δ(h(s)) = \frac{δ(s-s_0)}{\abs{h'(s)}} \] how on earth can he divide a delta distribution by something and saying it is equal to another delta distribution how can he do that. I'm dying inside. \[ h'(s) = \dpd{h}{s} = \frac{g}{βs} \] and so \[ s_0 = \mop{exp}\left(-\frac{β}{g} \left(\sum_{i=1}^N \frac{p_i^{'2}}{2m} + U(r^n)\right) + \frac{p_2^2}{2Q} + E \right)\]

Now he says that there's some $s^{3N}$ that comes out of the Jacobian of some change of variable in a non-derivable function. Seriously what the fuck. After more integrations he ends up with another horrible funciton and sets $g = 3N + 1$ to get nice constants.

Some real variables (with ') and virtual Nosé variables:

\begin{itemize}
	\item $r' = r$.
	\item $p' = \sfrac{p}{s}$.
	\item $Δt' = \sfrac{Δt}{s}$.
\end{itemize}

So equations of motion in real variables \[ E_{NH}^{const} = \sum \frac{p_i^2}{2m_i} + U(r^N) + \frac{\mathcal{H}_{NH}^2 Q}{2} + g \frac{\mathcal{H}_{NH}}{β} \]

Only static properties can be averaged with Nosé. Uniform sampling in real time $g = 3N$.

\chapter{Monte Carlo simulations}

Minimal standard by Park and Miller for random generators: given $a = 16807$, $m = 2^{31} - 1$, these values give an appropriate uniform distribution. However it is not good enough: there are residuals (plotting pair of numbers in 2D space show planes, which should not happed).

Another standard is \textit{Ran2}, which is a modified Park and Miller that gets rid of periods and residuals.

For the serial correlations, they use a shuffle algorithm. Given a series $\set{x_i}_{i=1}^∞$, we use a memory table of $32$ numbers to introduce disorder. Thus, when the random number generates the number $x_n$, there is a function $f$ that selects a place $j$ on the table $x_{n-1}$: the previous value in place $j$ is returned as the generator output and $x_n$ is replaced by $x_n$ in the table.

For increasing the period of the generator, they use a combination of different sequences. Given two congruential series $\set{x_n}_{n=1}^∞$, $\set{y_n}_{n=1}^∞$ with different periods $m_x - 1$ and $m_y - 1$, we construct a new series $z_n = (x_n - y_n) \mod m_x$ which will have as period $\mop{mcm}(m_x - 1, m_y - 1)$.

Finally, to avoid overflows  we have the Schrage algorithm. Given a modulo generator with parameters $m, a$, we can write $m = aq + r$ (a simple division, $q$ is the quotient and $r$ the remainder). Thus, in order to get the next value $x_{n + 1}$ based on the previous $x_{n-1}$, we can write \[ x_{n} = \floor{\sfrac{x_{n}}{q}} q + z \mod q\] so that
\begin{align*}
x_{n+1} &= ax_n \mod m = \\
	&= \left(\floor{\sfrac{x_{n}}{q}} aq + a · \left(x_n \mod q\right)\right) \mod m = \\
	&= \left(\floor{\sfrac{x_{n}}{q}} a(m - r) + a · \left(x_n \mod q\right)\right) \mod m = \\
	&= \left(a · \left(x_n \mod q\right) - r\floor{\sfrac{x_{n}}{q}} \right) \mod m
\end{align*}

It's easy to see that both values are less than $m$: $a (z \mod q) < aq ≤ m$ and $r \floor{\sfrac{x_n}{q}} ≤ ra < qa = m -r$. Thus, we can control the overflow of numbers. And $a \densein b$.

Then a shift register generator.

\section{Generating random distributions based on the uniform}

Given a random variable $X$ with uniform distribution of support $[0,1]$, we want to construct a random variable $Y$ with another distribution. After dividing by differentials\footnote{Again my degree in math slips away from my hands.} to look like we are doing math then we see that we need the $\mathrm{CDF}$ (which we will call $f$) of the target distribution so that $y = \inv{f}(x)$.

In the case of the Gaussian, the analytical part is a little bit complicated to get, so Box and Muller get a two-dimensional gaussian which is easier to compute. After doing yet another operation with differentials without integrations (why), it turns out we only need an uniform distribution with support $[0, 2π]$ and an exponential which can be calculated from an uniform supported $[0,1]$.

Another option is the rejection method from von Neumann. We have a PDF $f(x) ≥ 0$ without analytical way to get the CDF. Simple way: draw $v \sim \mop{Unif}(\sup f)$, $u \sim \mop{Unif}(0,1)$, if $u < f(v)$ accept the value of $v$, else reject it.

Also generate other things rejection and majorizing function.

\section{Random walks}

\section{Montecarlo integration}

\subsection{Introduction}

\subsection{Method}

\subsection{Scaling error}

Uncertainty goes proportional to the inverse of the square root of the number of points.

\[ σ_I^2 = \frac{1}{N} σ_f^2 \] where $σ_f$ is the width of the function (std dev wrt the mean).

\subsection{Importance sampling}

\appendix

\chapter{Evaluation methods}

There are two ways to get to the exam.

\paragraph{Choose a simulation project (Recommended)} Choose a project from the web (password \textit{simphys}). Follow two hours per week of the course for a general background. We get to the exam by handing in at the end of the semester (21\textsuperscript{st} December). Ten minutes of presentation of the project at the exam plus questions. Deadline for project approval is in two weeks (aka first week of October).

The project must be done with a person of the Faculty of Basic Sciences, in C/Fortran. It should be original research. The description must be one A4 page with the proposant (professor proposing, with name and address), the motivation behind the project, the methods envisioned/considered for the project, and the student's email and name. This has to be submitted through the webpage.

Two hard copies of the report, handed in in the professor's office or in the letterbox. It also has to be uploaded to Moodle, along with the code.

The report must have the following structure:

\begin{enumerate}
\item Introduction \& motivation. Why is the simulation is necessary and ffs it's like every other introduction.
\item Description of the model.
\item Implementation of the computer code.
\item Benchmarks.
\item Results + discussions.
\item Conclusions.
\end{enumerate}

The evaluation criteria is the following:

\begin{enumerate}
\item Choice of topic and level of challenge.
\item Achieved results: quality of the computational work and physical understanding of the problem.
\item Quality of the reports.
\item Quality of the oral presentation.
\item Answers to the questions (depth of understanding).
\item Expert present at the exam gives a feeling of the work.
\end{enumerate}

\paragraph{Exercise sessions} They give code, we do magic with the code. Then we have a report and then an exam with questions on the course.

%\chapter{---}
%\input{tex/ComputerSimulationOfPhysicalSystemsI_Exerc.tex}

\backmatter
\printindex
\end{document}
